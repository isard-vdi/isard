#!/usr/sbin/python3

#
#   Copyright © 2017-2025 Josep Maria Viñolas Auquer, Alberto Larraz Dalmases
#
#   This file is part of IsardVDI.
#
#   IsardVDI is free software: you can redistribute it and/or modify
#   it under the terms of the GNU Affero General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or (at your
#   option) any later version.
#
#   IsardVDI is distributed in the hope that it will be useful, but WITHOUT ANY
#   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
#   FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
#   details.
#
#   You should have received a copy of the GNU Affero General Public License
#   along with IsardVDI. If not, see <https://www.gnu.org/licenses/>.
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import argparse
import concurrent.futures
import json
import os
import subprocess
import threading
from datetime import datetime

from isardvdi_common.api_rest import ApiRest

lock = threading.Lock()

api = ApiRest()

roles = ["advanced", "manager", "admin"]


def get_storages_by_role(role):
    """Get all storages by role."""
    storages = api.get(
        f"/admin/storage/by-role/{role}",
        timeout=120,
    )
    storages = [
        storage
        for storage in storages
        if "directory_path" in storage
        and storage["status"] in ["ready", "recycled"]
        and storage["kind"] == ["desktop"]
    ]
    for storage in storages:
        storage["path"] = (
            f"{storage['directory_path']}/{storage['id']}.{storage['type']}"
        )
        storage["datetime"] = (
            datetime.fromtimestamp(storage["latest_status_time"]).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            if "latest_status_time" in storage
            else ""
        )
    return storages


def _is_valid_storage(storage):
    """Helper to check if a storage is valid and update its actual size from qemu-img info (uses JSON output)."""
    path = storage["path"]
    if not path.endswith(".qcow2"):
        return None
    try:
        result = subprocess.run(
            ["qemu-img", "info", "--output=json", "-U", "--backing-chain", path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
        )
        info = json.loads(result.stdout)
        # info is a list of dicts (backing chain), sum all 'actual-size'
        if isinstance(info, list):
            actual_size = info[0].get("actual-size", 0)
        else:
            actual_size = info.get("actual-size", 0)
        storage["actual-size"] = actual_size
        return storage
    except Exception:
        return None


def valid_storages(storages):
    """Check if storages are valid, end with .qcow2, and pass qemu-img info. Parallelized. Updates actual size.
    Returns (valid, invalid) storages lists.
    """
    valid = []
    invalid = []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(_is_valid_storage, storage): storage for storage in storages
        }
        total = len(futures)
        for idx, future in enumerate(concurrent.futures.as_completed(futures)):
            storage = futures[future]
            result = future.result()
            if result:
                valid.append(result)
            else:
                # print(f"{idx+1}/{total} - Invalid storage: {storage['path']}")
                invalid.append(storage)
    return valid, invalid


def save_to_file(filename, data, logs_path):
    """Save data to a file, each entry in a new line. Remove file if previously existed."""
    filepath = os.path.join(logs_path, filename)
    with lock:
        with open(filepath, "w") as file:
            file.write(data + "\n")


def report_storage_table(storages, role, logs_path):
    """Print a summary and detailed report of storages for a given role, and save as markdown."""
    if not storages:
        print(f"No storages found for role '{role}'.")
        return

    # Prepare summary data
    count = len(storages)
    total_size = 0
    oldest = None
    newest = None

    # Gather info for summary and details
    details = []
    for storage in storages:
        dt_str = storage.get("datetime", "")
        path = storage.get("path", "")
        actual_size = (
            storage.get("actual-size", 0) or storage.get("actual_size", 0) or 0
        )
        total_size += actual_size

        # Parse datetime for oldest/newest
        try:
            dt_obj = datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S") if dt_str else None
        except Exception:
            dt_obj = None

        if dt_obj:
            if not oldest or dt_obj < oldest[0]:
                oldest = (dt_obj, path)
            if not newest or dt_obj > newest[0]:
                newest = (dt_obj, path)

        details.append((dt_str, path, actual_size))

    # Save report as markdown
    md_lines = []
    resume_lines = []
    md_lines.append(f"# Storage Report for Role: `{role}`\n")
    md_lines.append("## Summary")
    md_lines.append(f"- **Count:** {count}")
    md_lines.append(f"- **Total actual size:** {total_size / (1024**3):.2f} GiB")
    if oldest:
        md_lines.append(
            f"- **Oldest:** {oldest[0].strftime('%Y-%m-%d %H:%M:%S')} - `{oldest[1]}`"
        )
    if newest:
        md_lines.append(
            f"- **Newest:** {newest[0].strftime('%Y-%m-%d %H:%M:%S')} - `{newest[1]}`"
        )
    resume_lines = md_lines.copy()
    md_lines.append("\n## Details\n")
    md_lines.append("| DATETIME             | ACTUAL-SIZE (GiB) | PATH |")
    md_lines.append("|----------------------|-------------------|------|")
    for dt_str, path, actual_size in details:
        md_lines.append(f"| {dt_str:20} | {actual_size / (1024**3):17.2f} | `{path}` |")

    md_report = "\n".join(md_lines)
    md_filename = os.path.join(logs_path, f"{role}_storages.md")
    with lock:
        with open(md_filename, "w") as f:
            f.write(md_report)
    return md_filename, resume_lines


if __name__ == "__main__":
    start_time = datetime.now()

    parser = argparse.ArgumentParser(description="Generate storage reports by role")
    parser.add_argument(
        "--base-path",
        default="/logs",
        help="Base path for /disks-by-role logs directory (default: /logs in docker /opt/isard-local/logs/storage in host)",
    )
    args = parser.parse_args()

    logs_path = os.path.join(args.base_path, "disks-by-role")
    os.makedirs(logs_path, exist_ok=True)

    print(
        f"[{start_time.strftime('%Y-%m-%d %H:%M:%S')}] Starting storage reports by role - Output: {logs_path}"
    )

    def format_size(size_bytes):
        """Convert bytes to human readable format."""
        if size_bytes == 0:
            return "0 B"
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    resume = []
    for role in roles:
        role_start_time = datetime.now()

        storages = get_storages_by_role(role)
        valid_storages_list, invalid_storages_list = valid_storages(storages)

        # Calculate total size for valid storages
        total_size = sum(
            storage.get("actual-size", 0) for storage in valid_storages_list
        )

        role_elapsed = datetime.now() - role_start_time
        print(
            f"[{role_start_time.strftime('%H:%M:%S')}] {role}: {len(storages)} total, {len(valid_storages_list)} valid ({format_size(total_size)}), {len(invalid_storages_list)} invalid - {role_elapsed.total_seconds():.1f}s"
        )

        save_to_file(
            f"invalid_storages_{role}.list",
            "\n".join([storage["path"] for storage in invalid_storages_list]),
            logs_path,
        )

        storage_paths = [storage["path"] for storage in valid_storages_list]
        save_to_file(f"{role}_storages.list", "\n".join(storage_paths), logs_path)
        md_filename, resume_lines = report_storage_table(
            valid_storages_list, role, logs_path
        )

        resume.extend(resume_lines)

    end_time = datetime.now()
    total_elapsed = end_time - start_time

    print(
        f"[{end_time.strftime('%Y-%m-%d %H:%M:%S')}] Completed in {total_elapsed.total_seconds():.1f}s - Reports: {logs_path}"
    )

    # Only show summary if there are results
    if resume:
        print("Summary:")
        for line in resume:
            if line.startswith("# Storage Report") or line.startswith("- **"):
                print(f"  {line}")
            if line.startswith("---"):
                break
