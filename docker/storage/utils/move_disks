#!/usr/bin/env python3

#
#   Copyright © 2017-2025 Josep Maria Viñolas Auquer, Alberto Larraz Dalmases
#
#   This file is part of IsardVDI.
#
#   IsardVDI is free software: you can redistribute it and/or modify
#   it under the terms of the GNU Affero General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or (at your
#   option) any later version.
#
#   IsardVDI is distributed in the hope that it will be useful, but WITHOUT ANY
#   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
#   FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
#   details.
#
#   You should have received a copy of the GNU Affero General Public License
#   along with IsardVDI. If not, see <https://www.gnu.org/licenses/>.
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import argparse
import os
import signal
import subprocess
import sys
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path
from pprint import pprint

from isardvdi_common.api_rest import ApiRest


def timestamp():
    """Get current timestamp string for logging."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def log(message):
    """Print message with timestamp prefix."""
    print(f"[{timestamp()}] {message}")


def ensure_data_folder():
    """Ensure the data folder exists."""
    os.makedirs("/logs", exist_ok=True)


def cleanup_bad_files_logs():
    """Remove bad files logs at startup (but keep dated moved files logs)."""
    bad_files_logs = [
        "non_existing_in_db.json",
        "not_ready.json",
        "non_existing_in_db_after_rsync.json",
        "not_ready_after_rsync.json",
        "failed_to_move_to_pool.json",
        "non_valid_qcow_files.json",
        "files_with_no_domains.json",
        "recycled_storage.json",
    ]

    for log_file in bad_files_logs:
        filepath = os.path.join("/logs", log_file)
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                print(f"Cleared log file: {log_file}")
            except Exception as e:
                print(f"Warning: Could not remove {log_file}: {e}")


ensure_data_folder()
lock = threading.Lock()


def save_to_file(filename, data):
    """Save data to a file, each entry in a new line."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name = filename.split(".")[0]
    extension = filename.split(".")[-1] if "." in filename else ""
    timestamped_filename = (
        f"move_disks-{timestamp}-{base_name}.{extension}"
        if extension
        else f"move_disks-{timestamp}-{base_name}"
    )
    filepath = os.path.join("/logs", timestamped_filename)
    with lock:
        with open(filepath, "a") as file:
            file.write(data + "\n")


def get_dated_moved_filename():
    """Get the filename for moved files log with today's date prefix."""
    today = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"move_disks-{today}-moved_to_pool.json"


### API calls


def get_storage_pools():
    """Get all storage pools from the system."""
    api = ApiRest()
    return api.get(
        "/admin/storage_pools",
        timeout=30,
    )


def get_storage_pool_by_name(pool_name):
    """Get storage pool information by name.

    Args:
        pool_name (str): Name of the storage pool to find

    Returns:
        dict: Storage pool information or None if not found
    """
    try:
        pools = get_storage_pools()
        for pool in pools:
            if pool.get("name", "").lower() == pool_name.lower():
                return pool
        return None
    except Exception as e:
        print(f"Error retrieving storage pool '{pool_name}': {e}")
        return None


def show_storage_pools_info():
    """Display storage pools with their name, mountpoint, and ID."""
    try:
        pools = get_storage_pools()
        if not pools:
            print("No storage pools found.")
            return

        # Filter only enabled pools
        enabled_pools = [pool for pool in pools if pool.get("enabled", False)]

        if not enabled_pools:
            print("No enabled storage pools found.")
            return

        print("Enabled Storage Pools:")
        print("-" * 80)
        print(f"{'Name':<20} {'Mountpoint':<30} {'ID':<36}")
        print("-" * 80)

        for pool in enabled_pools:
            name = pool.get("name", "N/A")
            mountpoint = pool.get("mountpoint", "N/A")
            pool_id = pool.get("id", "N/A")
            print(f"{name:<20} {mountpoint:<30} {pool_id:<36}")

        print("-" * 80)
        print(f"Total enabled pools: {len(enabled_pools)}")

    except Exception as e:
        print(f"Error retrieving storage pools: {e}")


def get_storage_statuses(storage_id):
    api = ApiRest()
    return api.get(
        f"/storage/{storage_id}/statuses",
        timeout=30,
    )


def get_all_recycled_storage():
    """
    Get all recycled storage in ONE API call for optimization.

    Returns:
        dict: Mapping of storage_id -> storage info for all recycled storage
    """
    api = ApiRest()
    try:
        storages = api.get(
            "/admin/storage/recycled",
            timeout=60,
        )
        # Create lookup dict for O(1) access: {storage_id: storage_info}
        return {s["id"]: s for s in storages}
    except Exception as e:
        print(f"Warning: Could not fetch recycled storage list: {e}")
        print("Falling back to individual API calls per file")
        return {}


def rsync_to_storage_pool(storage_id, storage_pool_id):
    global BWLIMIT
    api = ApiRest()
    data = {
        "destination_storage_pool_id": storage_pool_id,
        "remove_source_file": True,
        "bwlimit": BWLIMIT,
    }
    return api.put(
        f"/storage/{storage_id}/rsync/to-storage-pool",
        data=data,
        timeout=30,
    )


def wait_ready(storage_id, timeout_seconds, stop_requested=None):
    if timeout_seconds == 0:
        try:
            statuses = get_storage_statuses(storage_id)
        except Exception as e:
            return None
        ss = statuses["status"]
        if not len(statuses["domains"]):
            # For recycled storages without domains, just check storage status
            if ss == "recycled":
                print(f"Storage path {statuses['path']} has no domains (recycled storage)")
                return True
            # For other statuses, log and check if ready
            print(f"Storage path {statuses['path']} has no domains")
            save_to_file("files_with_no_domains.json", statuses["path"])
            if ss == "ready":
                return True
            return False
        ds = statuses["domains"][0]["status"]
        if ss in ["ready", "recycled"] and ds == "Stopped":
            return True
        return False

    start_time = time.time()
    last_warning_time = start_time
    WARNING_INTERVAL = 300  # Warn every 5 minutes

    while time.time() - start_time < timeout_seconds:
        elapsed = time.time() - start_time

        # Warn if stuck for more than 5 minutes
        if elapsed - (last_warning_time - start_time) >= WARNING_INTERVAL:
            log(f"WARNING: Still waiting for storage {storage_id} after {int(elapsed/60)} minutes...")
            last_warning_time = time.time()

        # Check if stop was requested (CTRL+C)
        if stop_requested and stop_requested.is_set():
            log(f"Stop requested - aborting wait for storage {storage_id}")
            return False  # Exit early, file will be logged as failed

        try:
            statuses = get_storage_statuses(storage_id)
        except Exception as e:
            return None

        ss = statuses["status"]
        if not len(statuses["domains"]):
            # For storages without domains (including recycled), check status
            if ss == "ready":
                return True
            elif ss in ["recycled", "maintenance"]:
                # Storage transitioning during rsync operation
                log(f"Waiting for rsync to complete for storage {storage_id} (status: {ss})...")
                time.sleep(5)
                continue  # Continue waiting, don't try to access domains[0]
            else:
                # Other unexpected status
                print(f"Storage path {statuses['path']} has no domains, unexpected status: {ss}")
                save_to_file("files_with_no_domains.json", statuses["path"])
                return False
        # If we reach here, storage has domains
        ds = statuses["domains"][0]["status"]
        if ss == "recycled":
            # This is an invalid state: recycled storage should not have domains
            print(f"WARNING: Storage {storage_id} is recycled but has domains attached (invalid state)")
            save_to_file("invalid_recycled_with_domains.json", {
                "storage_id": storage_id,
                "path": statuses["path"],
                "domain_id": statuses["domains"][0].get("id"),
                "domain_status": ds
            })
            # Continue anyway - treat like ready storage
            if ds == "Stopped":
                return True
        elif ss == "ready" and ds == "Stopped":
            return True
        time.sleep(5)
    return False


### File checks


def get_backing_chain(file_path):
    """Get the full backing chain for a QCOW2 image, suppressing output and errors."""
    try:
        result = subprocess.run(
            ["qemu-img", "info", "-U", "--backing-chain", file_path],
            stdout=subprocess.PIPE,  # Suppress stdout
            stderr=subprocess.PIPE,  # Suppress stderr
            text=True,
            check=True,
        )
        return result.stdout  # Return the result if no error
    except subprocess.CalledProcessError:
        return None


def is_valid_qcow2(file_absolute_path):
    """Check if the given file is a valid qcow2 file."""
    file = Path(file_absolute_path)
    if not file.is_file() or not file.name.endswith(".qcow2"):
        return False

    qemu_img = get_backing_chain(str(file))
    if qemu_img is not None:
        return True
    else:
        return False


def get_sorted_file_paths_by_date(iterate_path, filter_date, min_size_bytes=0, min_size_total_bytes=0, is_fast_pool=False, size_filter_type="none", check_recycled=False, recycled_files_list=None, destination_pool_path=None):
    """
    Collect and return file paths separated by modification date.

    Args:
        iterate_path (str): The path to iterate and collect files from.
        filter_date (str): ISO-format date string to filter files (e.g., "2025-01-01T00:00:00").
        min_size_bytes (int): Minimum file size in bytes for individual filtering (0 to disable).
        min_size_total_bytes (int): Minimum total size in bytes for cumulative filtering (0 to disable).
        is_fast_pool (bool): True if this is the fast pool (applies size filtering to old files).
        size_filter_type (str): Type of filtering ("none", "individual", "cumulative").
        check_recycled (bool): If True, check DB for recycled status and add to recycled_files_list.
        recycled_files_list (list): List to append (file_path, size) tuples for recycled files.
        destination_pool_path (str): Destination pool path (used to skip recycled files already in destination).

    Returns:
        tuple: (files_before_data, files_after_data)
               - files_before_data: (file_paths, total_size, filtered_count, filtered_size) sorted oldest first (for fast→slow migration)
               - files_after_data: (file_paths, total_size, filtered_count, filtered_size) sorted newest first (for slow→fast migration)
    """
    files_before = []
    files_after = []
    filter_dt = datetime.fromisoformat(filter_date)

    total = 0
    oldest_date = datetime.now()
    newest_date = datetime.fromtimestamp(0)
    total_size_before = 0
    total_size_after = 0
    filtered_count_before = 0
    filtered_size_before = 0

    # OPTIMIZATION: Fetch all recycled storage at once (instead of per-file API calls)
    recycled_storage_map = {}
    if check_recycled and recycled_files_list is not None:
        print("Fetching all recycled storage from API (optimization: single API call)...")
        recycled_storage_map = get_all_recycled_storage()
        print(f"Found {len(recycled_storage_map)} recycled storage entries in database")

    for file in Path(iterate_path).rglob("*"):
        if file.is_file():
            total += 1
            # Get the modification time and size of the file
            mtime = datetime.fromtimestamp(file.stat().st_mtime)
            file_size = file.stat().st_size

            # Check if file is recycled (for force-recycled mode) - OPTIMIZED with O(1) lookup
            if check_recycled and recycled_files_list is not None:
                storage_id = file.stem
                # O(1) dictionary lookup instead of API call per file!
                if storage_id in recycled_storage_map:
                    storage_info = recycled_storage_map[storage_id]
                    directory_path = storage_info.get("directory_path", "")

                    # Check if already in destination pool - skip if so
                    if destination_pool_path and directory_path.startswith(destination_pool_path):
                        print(f"Skipping recycled {file} - already in destination pool {destination_pool_path}")
                        continue  # Skip - already in destination

                    # Only include desktop usage (groups subdirectory) - vdo3 only supports desktop
                    if directory_path.endswith("/groups") or "/groups/" in directory_path:
                        recycled_files_list.append((str(file), file_size))
                        print(f"Found recycled desktop: {file} (will move to slow pool)")
                    else:
                        # Extract usage type from directory path for logging
                        usage_type = directory_path.split("/")[-1] if directory_path else "unknown"
                        print(f"Skipping recycled {file} - usage type '{usage_type}' not supported by destination pool (only 'groups'/desktop supported)")
                    continue  # Skip normal date classification

            # Separate files by date (no filtering yet)
            if mtime < filter_dt:
                files_before.append((file, mtime, file_size))
                total_size_before += file_size
            else:
                files_after.append((file, mtime, file_size))
                total_size_after += file_size

            # Track date range
            if mtime < oldest_date:
                oldest_date = mtime
            if mtime > newest_date:
                newest_date = mtime

    # Apply size filtering for fast→slow moves (old files from fast pool)
    files_to_move_before = []
    filtered_count_before = 0
    filtered_size_before = 0
    move_size_before = 0

    if is_fast_pool and len(files_before) > 0:
        if size_filter_type == "individual" and min_size_bytes > 0:
            # Individual file size filtering
            for file_data in files_before:
                file, mtime, file_size = file_data
                if file_size >= min_size_bytes:
                    files_to_move_before.append(file_data)
                    move_size_before += file_size
                else:
                    filtered_count_before += 1
                    filtered_size_before += file_size

        elif size_filter_type == "cumulative" and min_size_total_bytes > 0:
            # Cumulative size filtering - sort by size (smallest first), skip until threshold reached
            files_by_size = sorted(files_before, key=lambda x: x[2])  # Sort by file size
            cumulative_size = 0

            for file_data in files_by_size:
                file, mtime, file_size = file_data
                if cumulative_size < min_size_total_bytes:
                    # Still accumulating small files to skip
                    cumulative_size += file_size
                    filtered_count_before += 1
                    filtered_size_before += file_size
                else:
                    # Threshold reached, include this and all remaining files
                    files_to_move_before.append(file_data)
                    move_size_before += file_size

        else:
            # No filtering
            files_to_move_before = files_before
            move_size_before = total_size_before
    else:
        # Not fast pool, no filtering
        files_to_move_before = files_before
        move_size_before = total_size_before

    # Update totals for files that will actually be moved
    total_size_before = move_size_before
    files_before = files_to_move_before

    def format_size(size_bytes):
        """Convert bytes to human readable format."""
        if size_bytes == 0:
            return "0 B"
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    print(f"Total files: {total}")
    print(
        f"Files before {filter_date}: {len(files_before)} ({format_size(total_size_before)})"
    )
    if filtered_count_before > 0:
        if size_filter_type == "individual":
            print(
                f"  Filtered out (size < {min_size_bytes // (1024*1024)}MB): {filtered_count_before} ({format_size(filtered_size_before)})"
            )
        elif size_filter_type == "cumulative":
            print(
                f"  Filtered out (smallest files totaling {format_size(filtered_size_before)}): {filtered_count_before} files"
            )
    print(
        f"Files after {filter_date}: {len(files_after)} ({format_size(total_size_after)})"
    )
    print(f"Oldest file: {oldest_date}, Newest file: {newest_date}")

    # Sort files_before by modification time (OLDEST FIRST for fast→slow)
    files_before.sort(key=lambda x: x[1])

    # Sort files_after by modification time (NEWEST FIRST for slow→fast)
    files_after.sort(key=lambda x: x[1], reverse=True)

    # Extract only the file paths in sorted order
    sorted_paths_before = [str(file[0]) for file in files_before]
    sorted_paths_after = [str(file[0]) for file in files_after]

    return (sorted_paths_before, total_size_before, filtered_count_before, filtered_size_before), (
        sorted_paths_after,
        total_size_after,
        0,  # No filtering for after files
        0,  # No filtering for after files
    )


def move_discs_api(file_path, destination_pool_id, rsync_timeout=1000, min_size_bytes=0, is_fast_to_slow=False, stop_requested=None):
    """Move a disk file to the destination pool via API.

    Args:
        file_path (str): Path to the file to move
        destination_pool_id (str): ID of the destination storage pool
        rsync_timeout (int): Timeout in seconds for rsync operations
        min_size_bytes (int): Minimum file size in bytes for fast→slow moves (0 to disable)
        is_fast_to_slow (bool): True if this is a fast→slow migration
        stop_requested (threading.Event): Event to signal stop request (for graceful shutdown)
    """
    if is_valid_qcow2(file_path):
        # Check file size threshold for fast→slow moves
        if is_fast_to_slow and min_size_bytes > 0:
            try:
                file_size = Path(file_path).stat().st_size
                if file_size < min_size_bytes:
                    print(f"Skipping {file_path} - file size {file_size // (1024*1024)}MB below threshold {min_size_bytes // (1024*1024)}MB")
                    return
            except OSError as e:
                print(f"Warning: Could not get file size for {file_path}: {e}")
                # Continue with migration if we can't get file size

        storage_id = file_path.split("/")[-1].split(".")[0]

        # First check if storage exists and get its status
        try:
            statuses = get_storage_statuses(storage_id)
        except Exception as e:
            print(f"Storage {storage_id} does not exist in database")
            save_to_file("non_existing_in_db.json", file_path)
            return

        # Allow moving recycled storages
        if statuses.get("status") == "recycled":
            print(f"Moving recycled storage {file_path}")

        if statuses.get("status") == "deleted":
            print(f"Skipping {file_path} - storage status is 'deleted'")
            return

        print(f"Moving {file_path} to pool")
        status = wait_ready(storage_id, 0, stop_requested)
        if status is None:
            print(f"Storage {storage_id} does not exist in database when going to move")
            save_to_file("non_existing_in_db.json", file_path)
            return
        if status is False:
            print(
                f"Storage/Domain {storage_id} is not ready. Status: {statuses.get('status')}"
            )
            save_to_file("not_ready.json", file_path)
            return
        try:
            pprint(rsync_to_storage_pool(storage_id, destination_pool_id))
            status = wait_ready(storage_id, rsync_timeout, stop_requested)
            if status is None:
                print(f"WARNING!!! Storage {storage_id} does not exist after rsync")
                save_to_file("non_existing_in_db_after_rsync.json", file_path)
                return
            if status is False:
                print(
                    f"WARNING!!! Storage/Domain for storage {storage_id} is not ready after rsync"
                )
                save_to_file("not_ready_after_rsync.json", file_path)
                return
            save_to_file(get_dated_moved_filename(), file_path)
        except Exception as e:
            print(f"WARNING!!! Failed to move {file_path} to pool")
            print(e)
            save_to_file("failed_to_move_to_pool.json", file_path)

    else:
        print(f"Skipping {file_path} - not a valid qcow2 file")
        save_to_file("non_valid_qcow_files.json", file_path)


def move_discs_api_with_stats(file_path, destination_pool_id, rsync_timeout=1000, min_size_bytes=0, is_fast_to_slow=False, stats=None, worker_type=None, lock=None, stop_requested=None, active_rsyncs=None):
    """Move a disk file to the destination pool via API with statistics tracking.

    Args:
        file_path (str): Path to the file to move
        destination_pool_id (str): ID of the destination storage pool
        rsync_timeout (int): Timeout in seconds for rsync operations
        min_size_bytes (int): Minimum file size in bytes for fast→slow moves (0 to disable)
        is_fast_to_slow (bool): True if this is a fast→slow migration
        stats (dict): Statistics tracking dictionary
        worker_type (str): Type of worker ("fast_to_slow" or "slow_to_fast")
        lock (threading.Lock): Thread lock for synchronization
        stop_requested (threading.Event): Event to signal stop request (for graceful shutdown)
        active_rsyncs (dict): Shared dict to track active rsync operations

    Returns:
        bool: True if successful, False otherwise
    """
    if is_valid_qcow2(file_path):
        # Check file size threshold for fast→slow moves
        if is_fast_to_slow and min_size_bytes > 0:
            try:
                file_size = Path(file_path).stat().st_size
                if file_size < min_size_bytes:
                    print(f"Skipping {file_path} - file size {file_size // (1024*1024)}MB below threshold {min_size_bytes // (1024*1024)}MB")
                    with lock:
                        stats['skipped_size'][worker_type] += 1
                    return False
            except OSError as e:
                print(f"Warning: Could not get file size for {file_path}: {e}")
                # Continue with migration if we can't get file size

        storage_id = file_path.split("/")[-1].split(".")[0]

        # First check if storage exists and get its status
        try:
            statuses = get_storage_statuses(storage_id)
        except Exception as e:
            print(f"Storage {storage_id} does not exist in database")
            save_to_file("non_existing_in_db.json", file_path)
            with lock:
                stats['failed_db'][worker_type] += 1
            return False

        # Allow moving recycled storages
        if statuses.get("status") == "recycled":
            log(f"Moving recycled storage {file_path}")

        if statuses.get("status") == "deleted":
            log(f"Skipping {file_path} - storage status is 'deleted'")
            with lock:
                stats['skipped_deleted'][worker_type] += 1
            return False

        log(f"Moving {file_path} to pool")
        status = wait_ready(storage_id, 0, stop_requested)
        if status is None:
            log(f"Storage {storage_id} does not exist in database when going to move")
            save_to_file("non_existing_in_db.json", file_path)
            with lock:
                stats['failed_db'][worker_type] += 1
            return False
        if status is False:
            log(
                f"Storage/Domain {storage_id} is not ready. Status: {statuses.get('status')}"
            )
            save_to_file("not_ready.json", file_path)
            with lock:
                stats['failed_not_ready'][worker_type] += 1
            return False
        try:
            log(f"Task ID: {rsync_to_storage_pool(storage_id, destination_pool_id).get('task_id', 'N/A')}")

            # Track this rsync as active
            if active_rsyncs is not None:
                with lock:
                    active_rsyncs[storage_id] = {
                        "file_path": file_path,
                        "start_time": time.time()
                    }

            status = wait_ready(storage_id, rsync_timeout, stop_requested)

            # Remove from active rsyncs if completed successfully
            if status is True and active_rsyncs is not None:
                with lock:
                    if storage_id in active_rsyncs:
                        del active_rsyncs[storage_id]

            if status is None:
                log(f"WARNING!!! Storage {storage_id} does not exist after rsync")
                save_to_file("non_existing_in_db_after_rsync.json", file_path)
                with lock:
                    stats['failed_after_rsync'][worker_type] += 1
                return False
            if status is False:
                log(
                    f"WARNING!!! Storage/Domain for storage {storage_id} is not ready after rsync"
                )
                save_to_file("not_ready_after_rsync.json", file_path)
                with lock:
                    stats['failed_after_rsync'][worker_type] += 1
                return False
            save_to_file(get_dated_moved_filename(), file_path)
            return True
        except Exception as e:
            log(f"WARNING!!! Failed to move {file_path} to pool")
            log(str(e))
            save_to_file("failed_to_move_to_pool.json", file_path)
            with lock:
                stats['failed_rsync'][worker_type] += 1
            return False

    else:
        log(f"Skipping {file_path} - not a valid qcow2 file")
        save_to_file("non_valid_qcow_files.json", file_path)
        with lock:
            stats['skipped_invalid'][worker_type] += 1
        return False


def worker(
    queue,
    destination_pool_id,
    lock,
    progress_counter,
    total_files,
    stop_requested,
    rsync_timeout,
    worker_type,
    min_size_bytes=0,
    stats=None,
    current_file_tracker=None,
    active_rsyncs=None,
):
    """Worker function for processing files in threads.

    Args:
        queue (list): Queue of file paths to process
        destination_pool_id (str): ID of the destination storage pool
        lock (threading.Lock): Thread lock for synchronization
        progress_counter (list): Progress counter (mutable)
        total_files (int): Total number of files to process
        stop_requested (threading.Event): Event to signal stop request
        rsync_timeout (int): Timeout in seconds for rsync operations
        worker_type (str): Type of worker ("fast_to_slow" or "slow_to_fast")
        min_size_bytes (int): Minimum file size in bytes for fast→slow moves
        stats (dict): Statistics tracking dictionary
        current_file_tracker (dict): Shared dict to track current file being processed
        active_rsyncs (dict): Shared dict to track active rsync operations
    """
    while queue and not stop_requested.is_set():
        file_path = None
        with lock:
            if queue:
                file_path = queue.pop(0)
        if file_path:
            # Track current file
            thread_name = threading.current_thread().name
            if current_file_tracker is not None:
                with lock:
                    current_file_tracker[thread_name] = file_path
            file_start_time = time.time()
            try:
                file_size = Path(file_path).stat().st_size
            except OSError:
                file_size = 0

            is_fast_to_slow = "fast" in worker_type.lower() and "slow" in worker_type.lower()

            # Track the attempt
            with lock:
                stats['attempts'][worker_type] += 1

            # Try to move the file
            success = move_discs_api_with_stats(file_path, destination_pool_id, rsync_timeout, min_size_bytes, is_fast_to_slow, stats, worker_type, lock, stop_requested, active_rsyncs)

            file_end_time = time.time()
            file_duration = file_end_time - file_start_time

            with lock:
                # Clear current file from tracker
                if current_file_tracker is not None and thread_name in current_file_tracker:
                    del current_file_tracker[thread_name]

                progress_counter[0] += 1
                if success:
                    stats['bytes_moved'][worker_type] += file_size
                    stats['files_moved'][worker_type] += 1
                    stats['total_time'][worker_type] += file_duration

                # Calculate running statistics
                total_moved_files = stats['files_moved'][worker_type]
                total_moved_bytes = stats['bytes_moved'][worker_type]

                def format_size(size_bytes):
                    if size_bytes == 0:
                        return "0 B"
                    for unit in ["B", "KB", "MB", "GB", "TB"]:
                        if size_bytes < 1024.0:
                            return f"{size_bytes:.1f} {unit}"
                        size_bytes /= 1024.0
                    return f"{size_bytes:.1f} PB"

                def format_speed(bytes_per_second):
                    return format_size(bytes_per_second) + "/s"

                # Calculate average speed for this direction
                avg_speed = 0
                if stats['total_time'][worker_type] > 0:
                    avg_speed = total_moved_bytes / stats['total_time'][worker_type]

                # Calculate ETA based on files remaining
                files_remaining = total_files - progress_counter[0]
                eta_str = ""
                if total_moved_files > 0 and files_remaining > 0 and stats['total_time'][worker_type] > 0:
                    avg_time_per_file = stats['total_time'][worker_type] / total_moved_files
                    eta_seconds = avg_time_per_file * files_remaining

                    def format_eta(seconds):
                        if seconds < 60:
                            return f"{int(seconds)}s"
                        elif seconds < 3600:
                            return f"{int(seconds / 60)}m"
                        else:
                            hours = seconds / 3600
                            return f"{hours:.1f}h"

                    eta_str = f" | ETA: {format_eta(eta_seconds)}"

                status_line = f"[{worker_type}] Files: {progress_counter[0]}/{total_files} | "
                if success:
                    status_line += f"Moved: {total_moved_files} files ({format_size(total_moved_bytes)}) | "
                    status_line += f"Last: {format_size(file_size)} | Avg Speed: {format_speed(avg_speed)}{eta_str}"
                else:
                    failed_count = progress_counter[0] - total_moved_files
                    status_line += f"Moved: {total_moved_files}, Failed/Skipped: {failed_count} | Avg Speed: {format_speed(avg_speed)}{eta_str}"

                # Show currently processing files
                if current_file_tracker is not None and current_file_tracker:
                    active_files = [Path(f).name for f in current_file_tracker.values()]
                    status_line += f" | Active: {', '.join(active_files[:3])}"  # Show up to 3 active files
                    if len(active_files) > 3:
                        status_line += f" (+{len(active_files)-3} more)"

                log(status_line)


def signal_handler(sig, frame, stop_requested):
    """Signal handler for graceful shutdown."""
    print("CTRL+C detected. Waiting for threads to finish...")
    stop_requested.set()


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Bilateral storage migration tool for IsardVDI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --fast-pool fast --slow-pool vdo3 --filter-date 2024-01-01T00:00:00
  %(prog)s --fast-pool ssd --slow-pool hdd --past-days 30  # Files older than 30 days
  %(prog)s --past-days 7 --threads-fast-to-slow 2 --bandwidth-limit 100000
  %(prog)s --min-size-mb 1000 --past-days 14  # Skip files < 1GB for fast→slow
  %(prog)s --min-size-total-mb 1000000 --past-days 14  # Skip smallest 1TB worth of files
  %(prog)s --force-recycled --past-days 30  # Move recycled files + normal date-based migration
  %(prog)s --only-recycled  # Move ONLY recycled files (no regular date-based migration)
  %(prog)s --only-recycled --threads-fast-to-slow 2  # Move only recycled with limited threads
  %(prog)s --past-days 30 --limit-fast-to-slow 2048000  # Limit fast→slow to 2TB
  %(prog)s --limit-fast-to-slow 1024000 --limit-slow-to-fast 512000  # Limit both directions
  %(prog)s --list-pools  # Show available storage pools
  %(prog)s --dry-run --past-days 14  # Preview migration for files older than 14 days
        """
    )

    # Pool configuration
    parser.add_argument(
        "--fast-pool",
        type=str,
        default="fast",
        help="Name of the fast storage pool (default: fast)"
    )
    parser.add_argument(
        "--slow-pool",
        type=str,
        default="vdo3",
        help="Name of the slow storage pool (default: vdo3)"
    )

    # Date filtering (mutually exclusive options)
    date_group = parser.add_mutually_exclusive_group()
    date_group.add_argument(
        "--filter-date",
        type=str,
        help="Date threshold for file classification (ISO format: YYYY-MM-DDTHH:MM:SS)"
    )
    date_group.add_argument(
        "--past-days",
        type=int,
        help="Number of days ago from today to use as filter date (e.g., 30 for files older than 30 days)"
    )

    # Threading configuration
    parser.add_argument(
        "--threads-fast-to-slow",
        type=int,
        default=4,
        help="Number of threads for moving old files fast → slow (0 to disable) (default: 4)"
    )
    parser.add_argument(
        "--threads-slow-to-fast",
        type=int,
        default=3,
        help="Number of threads for moving recent files slow → fast (0 to disable) (default: 3)"
    )

    # Performance configuration
    parser.add_argument(
        "--bandwidth-limit",
        type=int,
        default=200000,
        help="Bandwidth limit in KB/s (0 for unlimited) (default: 200000)"
    )
    parser.add_argument(
        "--rsync-timeout",
        type=int,
        default=3600,
        help="Timeout in seconds for rsync operations (default: 3600)"
    )
    parser.add_argument(
        "--startup-delay",
        type=int,
        default=10,
        help="Seconds to wait before starting processing (default: 10)"
    )

    # File filtering options (mutually exclusive)
    size_group = parser.add_mutually_exclusive_group()
    size_group.add_argument(
        "--min-size-mb",
        type=int,
        default=0,
        help="Minimum file size in MB for fast→slow migration (0 to disable, only affects fast→slow moves) (default: 0)"
    )
    size_group.add_argument(
        "--min-size-total-mb",
        type=int,
        help="Skip smallest files until their total size reaches this threshold in MB (cumulative size filtering for fast→slow moves)"
    )

    # Migration size limits
    parser.add_argument(
        "--limit-fast-to-slow",
        type=int,
        default=0,
        help="Maximum total data to move from fast→slow in MB (0 for unlimited) (default: 0)"
    )
    parser.add_argument(
        "--limit-slow-to-fast",
        type=int,
        default=0,
        help="Maximum total data to move from slow→fast in MB (0 for unlimited) (default: 0)"
    )

    # Utility options
    parser.add_argument(
        "--list-pools",
        action="store_true",
        help="List available storage pools and exit"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show migration plan without executing (preview mode)"
    )
    parser.add_argument(
        "--no-cleanup",
        action="store_true",
        help="Skip cleanup of bad files logs at startup"
    )
    parser.add_argument(
        "--force-recycled",
        action="store_true",
        help="Move all recycled storage files to slow pool, bypassing date filters"
    )
    parser.add_argument(
        "--only-recycled",
        action="store_true",
        help="Move ONLY recycled files (no regular date-based migration). Implies --force-recycled"
    )

    return parser.parse_args()


def validate_date_format(date_string):
    """Validate and parse the date string."""
    try:
        datetime.fromisoformat(date_string)
        return True
    except ValueError:
        return False


def main():
    """Main function to execute the bilateral storage migration process."""

    # Parse command line arguments
    args = parse_arguments()

    # Handle --only-recycled implying --force-recycled
    if args.only_recycled:
        args.force_recycled = True

    # Handle list-pools option
    if args.list_pools:
        print("=== AVAILABLE STORAGE POOLS ===")
        show_storage_pools_info()
        return

    # Handle date filtering options
    if args.past_days is not None:
        # Calculate filter date from past days
        if args.past_days < 0:
            print("ERROR: Past days cannot be negative")
            sys.exit(1)

        filter_date = datetime.now() - timedelta(days=args.past_days)
        FILTER_DATE = filter_date.strftime("%Y-%m-%dT%H:%M:%S")
        print(f"Using calculated filter date from {args.past_days} days ago: {FILTER_DATE}")
    elif args.filter_date is not None:
        # Use provided filter date
        if not validate_date_format(args.filter_date):
            print(f"ERROR: Invalid date format '{args.filter_date}'. Use ISO format: YYYY-MM-DDTHH:MM:SS")
            sys.exit(1)
        FILTER_DATE = args.filter_date
    else:
        # Default to 30 days ago if neither option is provided
        filter_date = datetime.now() - timedelta(days=30)
        FILTER_DATE = filter_date.strftime("%Y-%m-%dT%H:%M:%S")
        print(f"No date filter specified. Using default of 30 days ago: {FILTER_DATE}")

    # Validate thread counts
    if args.threads_fast_to_slow < 0 or args.threads_slow_to_fast < 0:
        print("ERROR: Thread counts cannot be negative")
        sys.exit(1)

    if args.threads_fast_to_slow == 0 and args.threads_slow_to_fast == 0:
        print("ERROR: At least one migration direction must be enabled (threads > 0)")
        sys.exit(1)

    # Validate bandwidth limit
    if args.bandwidth_limit < 0:
        print("ERROR: Bandwidth limit cannot be negative")
        sys.exit(1)

    # Validate minimum size parameters
    if args.min_size_mb is not None and args.min_size_mb < 0:
        print("ERROR: Minimum file size cannot be negative")
        sys.exit(1)

    if args.min_size_total_mb is not None and args.min_size_total_mb < 0:
        print("ERROR: Minimum total size cannot be negative")
        sys.exit(1)

    # Validate migration size limits
    if args.limit_fast_to_slow < 0:
        print("ERROR: Fast→slow limit cannot be negative")
        sys.exit(1)

    if args.limit_slow_to_fast < 0:
        print("ERROR: Slow→fast limit cannot be negative")
        sys.exit(1)

    # =============================================================================
    # STARTUP CLEANUP
    # =============================================================================

    if not args.no_cleanup:
        print("=== STARTUP CLEANUP ===")
        cleanup_bad_files_logs()
        print()

    # =============================================================================
    # CONFIGURATION FROM CLI ARGUMENTS
    # =============================================================================

    # Storage Pool Names (from CLI arguments)
    FAST_POOL_NAME = args.fast_pool
    SLOW_POOL_NAME = args.slow_pool

    # FILTER_DATE is already set above based on CLI arguments

    # Threading Configuration (from CLI arguments)
    MAX_THREADS_FAST_TO_SLOW = args.threads_fast_to_slow
    MAX_THREADS_SLOW_TO_FAST = args.threads_slow_to_fast
    BANDWIDTH_LIMIT = args.bandwidth_limit

    # Processing Configuration (from CLI arguments)
    STARTUP_DELAY = args.startup_delay
    RSYNC_TIMEOUT = args.rsync_timeout

    # Size filtering configuration
    if args.min_size_mb is not None and args.min_size_mb > 0:
        MIN_SIZE_BYTES = args.min_size_mb * 1024 * 1024  # Convert MB to bytes
        MIN_SIZE_TOTAL_BYTES = 0
        SIZE_FILTER_TYPE = "individual"
    elif args.min_size_total_mb is not None:
        MIN_SIZE_BYTES = 0
        MIN_SIZE_TOTAL_BYTES = args.min_size_total_mb * 1024 * 1024  # Convert MB to bytes
        SIZE_FILTER_TYPE = "cumulative"
    else:
        MIN_SIZE_BYTES = 0
        MIN_SIZE_TOTAL_BYTES = 0
        SIZE_FILTER_TYPE = "none"

    # =============================================================================
    # DYNAMIC POOL RESOLUTION
    # =============================================================================

    print("=== RESOLVING STORAGE POOLS ===")

    # Get fast pool information
    fast_pool = get_storage_pool_by_name(FAST_POOL_NAME)
    if not fast_pool:
        print(f"ERROR: Fast storage pool '{FAST_POOL_NAME}' not found!")
        return

    # Get slow pool information
    slow_pool = get_storage_pool_by_name(SLOW_POOL_NAME)
    if not slow_pool:
        print(f"ERROR: Slow storage pool '{SLOW_POOL_NAME}' not found!")
        return

    # Extract pool information
    FAST_POOL_ID = fast_pool["id"]
    FAST_POOL_PATH = fast_pool["mountpoint"]
    SLOW_POOL_ID = slow_pool["id"]
    SLOW_POOL_PATH = slow_pool["mountpoint"]

    print(f"Fast Pool: {FAST_POOL_NAME}")
    print(f"  ID: {FAST_POOL_ID}")
    print(f"  Path: {FAST_POOL_PATH}")
    print(f"  Enabled: {fast_pool.get('enabled', False)}")
    print()
    print(f"Slow Pool: {SLOW_POOL_NAME}")
    print(f"  ID: {SLOW_POOL_ID}")
    print(f"  Path: {SLOW_POOL_PATH}")
    print(f"  Enabled: {slow_pool.get('enabled', False)}")
    print()

    # Check if pools are enabled
    if not fast_pool.get("enabled", False):
        print(f"WARNING: Fast pool '{FAST_POOL_NAME}' is not enabled!")
    if not slow_pool.get("enabled", False):
        print(f"WARNING: Slow pool '{SLOW_POOL_NAME}' is not enabled!")

    # =============================================================================
    # MAIN EXECUTION
    # =============================================================================

    # Set global bandwidth limit
    global BWLIMIT
    BWLIMIT = BANDWIDTH_LIMIT

    if args.only_recycled:
        print("=== RECYCLED FILES ONLY MIGRATION ===")
        print(f"Mode: ONLY moving recycled files (date-based migration DISABLED)")
        print(f"Target: All recycled files → {SLOW_POOL_NAME} pool")
        print()
    else:
        print("=== BILATERAL STORAGE MIGRATION ===")
        print(f"Date threshold: {FILTER_DATE}")
        print(f"Files before date: {FAST_POOL_NAME} → {SLOW_POOL_NAME} pool (oldest first)")
        print(f"Files after date: {SLOW_POOL_NAME} → {FAST_POOL_NAME} pool (newest first)")
        print()

    # Initialize recycled files lists for force-recycled mode
    recycled_from_fast = []
    recycled_from_slow = []

    # Get file paths from both pools
    print(f"Scanning {FAST_POOL_NAME} pool ({FAST_POOL_PATH})...")
    fast_result = get_sorted_file_paths_by_date(FAST_POOL_PATH, FILTER_DATE, MIN_SIZE_BYTES, MIN_SIZE_TOTAL_BYTES, True, SIZE_FILTER_TYPE, check_recycled=args.force_recycled, recycled_files_list=recycled_from_fast, destination_pool_path=SLOW_POOL_PATH if args.force_recycled else None)
    fast_files_before, fast_size_before, fast_filtered_count, fast_filtered_size = fast_result[0]
    fast_files_after, fast_size_after, _, _ = fast_result[1]

    print(f"Scanning {SLOW_POOL_NAME} pool ({SLOW_POOL_PATH})...")
    slow_result = get_sorted_file_paths_by_date(SLOW_POOL_PATH, FILTER_DATE, 0, 0, False, "none", check_recycled=args.force_recycled, recycled_files_list=recycled_from_slow, destination_pool_path=SLOW_POOL_PATH if args.force_recycled else None)
    slow_files_before, slow_size_before, _, _ = slow_result[0]
    slow_files_after, slow_size_after, _, _ = slow_result[1]

    # Display force-recycled mode info
    if args.force_recycled and (recycled_from_fast or recycled_from_slow):
        recycled_size_fast = sum(f[1] for f in recycled_from_fast)
        recycled_size_slow = sum(f[1] for f in recycled_from_slow)

        def format_size(size_bytes):
            """Convert bytes to human readable format."""
            if size_bytes == 0:
                return "0 B"
            for unit in ["B", "KB", "MB", "GB", "TB"]:
                if size_bytes < 1024.0:
                    return f"{size_bytes:.1f} {unit}"
                size_bytes /= 1024.0
            return f"{size_bytes:.1f} PB"

        print(f"\n=== FORCE RECYCLED MODE ===")
        print(f"Found {len(recycled_from_fast)} recycled files in {FAST_POOL_NAME} ({format_size(recycled_size_fast)})")
        print(f"Found {len(recycled_from_slow)} recycled files in {SLOW_POOL_NAME} ({format_size(recycled_size_slow)})")
        print(f"All will be moved to {SLOW_POOL_NAME} pool (bypassing date filters)\n")

    def format_size(size_bytes):
        """Convert bytes to human readable format."""
        if size_bytes == 0:
            return "0 B"
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    # Prepare migration queues
    if args.only_recycled:
        # Only-recycled mode: Skip regular files, only process recycled files
        fast_to_slow_queue = []
        slow_to_fast_queue = []
        fast_size_before = 0
        slow_size_after = 0
    else:
        # Normal mode: Include regular date-based files
        # Old files from fast pool → slow pool
        fast_to_slow_queue = fast_files_before.copy()
        # Recent files from slow pool → fast pool
        slow_to_fast_queue = slow_files_after.copy()

    # Add recycled files to fast→slow queue (force-recycled mode)
    if args.force_recycled:
        recycled_paths_fast = [f[0] for f in recycled_from_fast]
        recycled_paths_slow = [f[0] for f in recycled_from_slow]
        recycled_size_fast = sum(f[1] for f in recycled_from_fast)
        recycled_size_slow = sum(f[1] for f in recycled_from_slow)

        if recycled_from_fast:
            fast_to_slow_queue.extend(recycled_paths_fast)
            fast_size_before += recycled_size_fast
        if recycled_from_slow:
            fast_to_slow_queue.extend(recycled_paths_slow)
            fast_size_before += recycled_size_slow

    # Apply migration size limits
    limit_fast_to_slow_bytes = args.limit_fast_to_slow * 1024 * 1024  # Convert MB to bytes
    limit_slow_to_fast_bytes = args.limit_slow_to_fast * 1024 * 1024  # Convert MB to bytes

    excluded_by_limit_fast = 0
    excluded_by_limit_slow = 0
    excluded_size_fast = 0
    excluded_size_slow = 0

    if limit_fast_to_slow_bytes > 0 and fast_to_slow_queue:
        original_count = len(fast_to_slow_queue)
        original_size = fast_size_before
        limited_queue = []
        cumulative_size = 0

        for file_path in fast_to_slow_queue:
            try:
                file_size = Path(file_path).stat().st_size
                if cumulative_size + file_size <= limit_fast_to_slow_bytes:
                    limited_queue.append(file_path)
                    cumulative_size += file_size
                # Once limit reached, stop adding files
            except OSError:
                # If we can't get file size, skip it
                pass

        # Update queue and calculate exclusions
        excluded_by_limit_fast = original_count - len(limited_queue)
        excluded_size_fast = original_size - cumulative_size
        fast_to_slow_queue = limited_queue
        fast_size_before = cumulative_size

    if limit_slow_to_fast_bytes > 0 and slow_to_fast_queue:
        original_count = len(slow_to_fast_queue)
        original_size = slow_size_after
        limited_queue = []
        cumulative_size = 0

        for file_path in slow_to_fast_queue:
            try:
                file_size = Path(file_path).stat().st_size
                if cumulative_size + file_size <= limit_slow_to_fast_bytes:
                    limited_queue.append(file_path)
                    cumulative_size += file_size
                # Once limit reached, stop adding files
            except OSError:
                # If we can't get file size, skip it
                pass

        # Update queue and calculate exclusions
        excluded_by_limit_slow = original_count - len(limited_queue)
        excluded_size_slow = original_size - cumulative_size
        slow_to_fast_queue = limited_queue
        slow_size_after = cumulative_size

    # Initialize threading variables
    all_threads = []
    stop_requested = threading.Event()

    # Progress counters
    fast_to_slow_progress = [0]
    slow_to_fast_progress = [0]

    # Current file trackers (shared between threads)
    current_file_tracker_fast = {}
    current_file_tracker_slow = {}

    # Active rsync tracker (tracks rsyncs in progress for graceful shutdown)
    active_rsyncs = {}

    total_fast_to_slow = len(fast_to_slow_queue)
    total_slow_to_fast = len(slow_to_fast_queue)

    # Calculate comprehensive totals
    # Fast pool totals (all files before + after the date threshold)
    fast_total_files_scanned = len(fast_files_before) + fast_filtered_count + len(fast_files_after)
    fast_total_size_scanned = fast_size_before + fast_filtered_size + fast_size_after

    # Slow pool totals
    slow_total_files_scanned = len(slow_files_before) + len(slow_files_after)
    slow_total_size_scanned = slow_size_before + slow_size_after

    # Overall totals
    total_files_found = fast_total_files_scanned + slow_total_files_scanned
    total_size_found = fast_total_size_scanned + slow_total_size_scanned

    total_files_to_move = total_fast_to_slow + total_slow_to_fast
    total_size_to_move = fast_size_before + slow_size_after

    total_files_excluded = fast_filtered_count  # Only fast→slow has exclusions
    total_size_excluded = fast_filtered_size

    print(f"\n" + "="*80)
    print("COMPREHENSIVE MIGRATION SUMMARY")
    print("="*80)

    print(f"TOTAL FILES FOUND: {total_files_found} files ({format_size(total_size_found)})")
    print(f"  ├─ {FAST_POOL_NAME} pool: {fast_total_files_scanned} files ({format_size(fast_total_size_scanned)})")
    print(f"  └─ {SLOW_POOL_NAME} pool: {slow_total_files_scanned} files ({format_size(slow_total_size_scanned)})")
    print()

    print(f"FILES TO BE MOVED: {total_files_to_move} files ({format_size(total_size_to_move)})")
    print(f"  ├─ {FAST_POOL_NAME} → {SLOW_POOL_NAME}: {total_fast_to_slow} files ({format_size(fast_size_before)})")
    print(f"  └─ {SLOW_POOL_NAME} → {FAST_POOL_NAME}: {total_slow_to_fast} files ({format_size(slow_size_after)})")
    print()

    # Show recycled files info if force-recycled mode is enabled
    if args.force_recycled and (recycled_from_fast or recycled_from_slow):
        total_recycled = len(recycled_from_fast) + len(recycled_from_slow)
        total_recycled_size = sum(f[1] for f in recycled_from_fast) + sum(f[1] for f in recycled_from_slow)
        print(f"RECYCLED FILES (FORCED): {total_recycled} files ({format_size(total_recycled_size)})")
        print(f"  ├─ From {FAST_POOL_NAME}: {len(recycled_from_fast)} files")
        print(f"  └─ From {SLOW_POOL_NAME}: {len(recycled_from_slow)} files")
        print(f"  → All moved to {SLOW_POOL_NAME} (bypassing date filters)")
        print()

    # Update total exclusions to include limit-based exclusions
    total_files_excluded = fast_filtered_count + excluded_by_limit_fast + excluded_by_limit_slow
    total_size_excluded = fast_filtered_size + excluded_size_fast + excluded_size_slow

    if total_files_excluded > 0:
        print(f"FILES EXCLUDED: {total_files_excluded} files ({format_size(total_size_excluded)})")
        if SIZE_FILTER_TYPE == "individual":
            print(f"  ├─ Individual size < {MIN_SIZE_BYTES // (1024*1024)}MB (fast→slow only): {fast_filtered_count} files ({format_size(fast_filtered_size)})")
        elif SIZE_FILTER_TYPE == "cumulative":
            print(f"  ├─ Smallest files totaling {format_size(fast_filtered_size)} (fast→slow only): {fast_filtered_count} files")

        if excluded_by_limit_fast > 0:
            print(f"  ├─ Exceeds fast→slow limit: {excluded_by_limit_fast} files ({format_size(excluded_size_fast)})")
        if excluded_by_limit_slow > 0:
            print(f"  └─ Exceeds slow→fast limit: {excluded_by_limit_slow} files ({format_size(excluded_size_slow)})")
        elif SIZE_FILTER_TYPE != "none":
            # Change last item marker if no limit exclusions
            pass
        print()

    print(f"PROCESSING CONFIGURATION:")
    print(f"  ├─ Date threshold: {FILTER_DATE}")
    print(f"  ├─ Max threads {FAST_POOL_NAME}→{SLOW_POOL_NAME}: {MAX_THREADS_FAST_TO_SLOW}")
    print(f"  ├─ Max threads {SLOW_POOL_NAME}→{FAST_POOL_NAME}: {MAX_THREADS_SLOW_TO_FAST}")
    print(f"  ├─ Bandwidth limit: {BANDWIDTH_LIMIT} KB/s")

    # Size filtering
    if SIZE_FILTER_TYPE == "individual":
        print(f"  ├─ Individual size threshold: {MIN_SIZE_BYTES // (1024*1024)} MB")
    elif SIZE_FILTER_TYPE == "cumulative":
        print(f"  ├─ Cumulative size threshold: {MIN_SIZE_TOTAL_BYTES // (1024*1024)} MB")
    else:
        print(f"  ├─ Size filtering: disabled")

    # Migration limits
    if args.limit_fast_to_slow > 0 and args.limit_slow_to_fast > 0:
        print(f"  ├─ Migration limit {FAST_POOL_NAME}→{SLOW_POOL_NAME}: {args.limit_fast_to_slow} MB ({format_size(limit_fast_to_slow_bytes)})")
        print(f"  └─ Migration limit {SLOW_POOL_NAME}→{FAST_POOL_NAME}: {args.limit_slow_to_fast} MB ({format_size(limit_slow_to_fast_bytes)})")
    elif args.limit_fast_to_slow > 0:
        print(f"  └─ Migration limit {FAST_POOL_NAME}→{SLOW_POOL_NAME}: {args.limit_fast_to_slow} MB ({format_size(limit_fast_to_slow_bytes)})")
    elif args.limit_slow_to_fast > 0:
        print(f"  └─ Migration limit {SLOW_POOL_NAME}→{FAST_POOL_NAME}: {args.limit_slow_to_fast} MB ({format_size(limit_slow_to_fast_bytes)})")
    else:
        print(f"  └─ Migration limits: disabled")

    print("="*80)

    # Handle dry-run mode
    if args.dry_run:
        print("\n=== DRY RUN MODE - NO FILES WILL BE MOVED ===")
        print("Migration plan completed. Use without --dry-run to execute.")
        return

    # Setup signal handler for graceful shutdown
    signal.signal(
        signal.SIGINT, lambda sig, frame: signal_handler(sig, frame, stop_requested)
    )

    print(f"\nStarting in {STARTUP_DELAY} seconds... (Press CTRL+C to cancel)")

    # Countdown with ability to cancel
    for remaining in range(STARTUP_DELAY, 0, -1):
        if stop_requested.is_set():
            print("\nStartup cancelled by user.")
            return
        print(
            f"\rStarting in {remaining} seconds... (Press CTRL+C to cancel)",
            end="",
            flush=True,
        )
        time.sleep(1)

    print("\rStarting migration now...                                    ")

    # Check one more time if user cancelled during countdown
    if stop_requested.is_set():
        print("Startup cancelled by user.")
        return

    # Start timing
    migration_start_time = time.time()
    initial_data_size = fast_size_before + slow_size_after

    # Initialize statistics tracking
    fast_to_slow_name = f"{FAST_POOL_NAME}→{SLOW_POOL_NAME}"
    slow_to_fast_name = f"{SLOW_POOL_NAME}→{FAST_POOL_NAME}"

    stats = {
        'attempts': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'files_moved': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'bytes_moved': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'total_time': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'skipped_size': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'skipped_recycled': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'skipped_deleted': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'skipped_invalid': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'failed_db': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'failed_not_ready': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'failed_after_rsync': {fast_to_slow_name: 0, slow_to_fast_name: 0},
        'failed_rsync': {fast_to_slow_name: 0, slow_to_fast_name: 0},
    }

    # Store configuration metadata for final statistics
    migration_config = {
        'force_recycled': args.force_recycled,
        'recycled_count_fast': len(recycled_from_fast) if args.force_recycled else 0,
        'recycled_count_slow': len(recycled_from_slow) if args.force_recycled else 0,
        'size_filter_type': SIZE_FILTER_TYPE,
        'min_size_bytes': MIN_SIZE_BYTES,
        'min_size_total_bytes': MIN_SIZE_TOTAL_BYTES,
        'filtered_count': fast_filtered_count,
        'filtered_size': fast_filtered_size,
        'filter_date': FILTER_DATE,
    }

    # Start worker threads for fast → slow migration
    if total_fast_to_slow > 0:
        if MAX_THREADS_FAST_TO_SLOW > 0:
            print(
                f"Starting {MAX_THREADS_FAST_TO_SLOW} threads for {FAST_POOL_NAME} → {SLOW_POOL_NAME} migration"
            )
            for i in range(MAX_THREADS_FAST_TO_SLOW):
                thread = threading.Thread(
                    target=worker,
                    args=(
                        fast_to_slow_queue,
                        SLOW_POOL_ID,
                        lock,
                        fast_to_slow_progress,
                        total_fast_to_slow,
                        stop_requested,
                        RSYNC_TIMEOUT,
                        fast_to_slow_name,
                        MIN_SIZE_BYTES,
                        stats,
                        current_file_tracker_fast,
                        active_rsyncs,
                    ),
                    name=f"FastToSlow-{i+1}",
                )
                all_threads.append(thread)
                thread.start()
        else:
            print(
                f"Migration {FAST_POOL_NAME} → {SLOW_POOL_NAME} is DISABLED (MAX_THREADS_FAST_TO_SLOW = 0)"
            )

    # Start worker threads for slow → fast migration
    if total_slow_to_fast > 0:
        if MAX_THREADS_SLOW_TO_FAST > 0:
            print(
                f"Starting {MAX_THREADS_SLOW_TO_FAST} threads for {SLOW_POOL_NAME} → {FAST_POOL_NAME} migration"
            )
            for i in range(MAX_THREADS_SLOW_TO_FAST):
                thread = threading.Thread(
                    target=worker,
                    args=(
                        slow_to_fast_queue,
                        FAST_POOL_ID,
                        lock,
                        slow_to_fast_progress,
                        total_slow_to_fast,
                        stop_requested,
                        RSYNC_TIMEOUT,
                        slow_to_fast_name,
                        0,  # No size threshold for slow→fast moves
                        stats,
                        current_file_tracker_slow,
                        active_rsyncs,
                    ),
                    name=f"SlowToFast-{i+1}",
                )
                all_threads.append(thread)
                thread.start()
        else:
            print(
                f"Migration {SLOW_POOL_NAME} → {FAST_POOL_NAME} is DISABLED (MAX_THREADS_SLOW_TO_FAST = 0)"
            )

    # Wait for all threads to finish
    for thread in all_threads:
        thread.join()

    # Wait for any in-progress rsync tasks to complete (after CTRL+C)
    if active_rsyncs:
        log(f"\nWaiting for {len(active_rsyncs)} active rsync task(s) to complete...")
        for storage_id, info in list(active_rsyncs.items()):
            log(f"  - {Path(info['file_path']).name}")

        while active_rsyncs:
            remaining = len(active_rsyncs)
            log(f"Waiting for rsync completion: {remaining} file(s) remaining...")

            for storage_id in list(active_rsyncs.keys()):
                try:
                    statuses = get_storage_statuses(storage_id)
                    ss = statuses["status"]

                    # Check if rsync completed (ready or recycled status)
                    if ss in ["ready", "recycled"]:
                        file_path = Path(active_rsyncs[storage_id]["file_path"]).name
                        log(f"Rsync completed for {file_path}")
                        del active_rsyncs[storage_id]
                except Exception as e:
                    # If storage no longer exists, remove from tracking
                    log(f"Storage {storage_id} no longer exists, removing from tracking")
                    if storage_id in active_rsyncs:
                        del active_rsyncs[storage_id]

            if active_rsyncs:
                time.sleep(5)  # Poll every 5 seconds

        log("All rsync tasks completed\n")

    # Calculate final statistics
    migration_end_time = time.time()
    total_migration_time = migration_end_time - migration_start_time

    def format_time(seconds):
        """Convert seconds to human readable format."""
        if seconds < 60:
            return f"{seconds:.1f}s"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}m"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}h"

    def format_size(size_bytes):
        """Convert bytes to human readable format."""
        if size_bytes == 0:
            return "0 B"
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    def format_speed(bytes_per_second):
        """Convert bytes per second to human readable format."""
        return format_size(bytes_per_second) + "/s"

    print("\n" + "="*80)
    print("MIGRATION COMPLETE - FINAL STATISTICS")
    print("="*80)

    print(f"Total Migration Time: {format_time(total_migration_time)}")
    print(f"Migration Start: {datetime.fromtimestamp(migration_start_time).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Migration End: {datetime.fromtimestamp(migration_end_time).strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # Calculate estimated vs actual comparison
    estimated_files_total = total_fast_to_slow + total_slow_to_fast
    estimated_size_total = initial_data_size
    actual_files_total = stats['files_moved'][fast_to_slow_name] + stats['files_moved'][slow_to_fast_name]
    actual_size_total = stats['bytes_moved'][fast_to_slow_name] + stats['bytes_moved'][slow_to_fast_name]

    print("--- ESTIMATED vs ACTUAL COMPARISON ---")
    print(f"Estimated files to move: {estimated_files_total}")
    print(f"Actual files moved: {actual_files_total}")
    if estimated_files_total > 0:
        file_accuracy = (actual_files_total / estimated_files_total) * 100
        print(f"File migration accuracy: {file_accuracy:.1f}%")

    print(f"Estimated data to move: {format_size(estimated_size_total)}")
    print(f"Actual data moved: {format_size(actual_size_total)}")
    if estimated_size_total > 0:
        size_accuracy = (actual_size_total / estimated_size_total) * 100
        print(f"Data migration accuracy: {size_accuracy:.1f}%")

    # Show the difference
    file_diff = actual_files_total - estimated_files_total
    size_diff = actual_size_total - estimated_size_total
    print(f"File difference: {file_diff:+d} files")
    print(f"Size difference: {format_size(abs(size_diff))} {'more' if size_diff >= 0 else 'less'}")
    print()

    # Show migration configuration details
    print("--- MIGRATION CONFIGURATION ---")
    print(f"Date filter: {migration_config['filter_date']}")
    if migration_config['force_recycled']:
        total_recycled = migration_config['recycled_count_fast'] + migration_config['recycled_count_slow']
        print(f"Force recycled mode: ENABLED ({total_recycled} recycled files targeted)")
        print(f"  ├─ From {FAST_POOL_NAME}: {migration_config['recycled_count_fast']} files")
        print(f"  └─ From {SLOW_POOL_NAME}: {migration_config['recycled_count_slow']} files")
    else:
        print(f"Force recycled mode: DISABLED")

    if migration_config['size_filter_type'] == "individual":
        print(f"Size filtering: Individual files < {migration_config['min_size_bytes'] // (1024*1024)} MB excluded")
        print(f"  └─ Filtered out: {migration_config['filtered_count']} files ({format_size(migration_config['filtered_size'])})")
    elif migration_config['size_filter_type'] == "cumulative":
        print(f"Size filtering: Smallest files totaling {format_size(migration_config['min_size_total_bytes'])} excluded")
        print(f"  └─ Filtered out: {migration_config['filtered_count']} files ({format_size(migration_config['filtered_size'])})")
    else:
        print(f"Size filtering: DISABLED")
    print()

    # Summary by direction
    for direction in [fast_to_slow_name, slow_to_fast_name]:
        print(f"--- {direction} ---")
        attempts = stats['attempts'][direction]
        moved = stats['files_moved'][direction]
        bytes_moved = stats['bytes_moved'][direction]
        avg_time = stats['total_time'][direction] / moved if moved > 0 else 0

        # Get estimated values for this direction
        if direction == fast_to_slow_name:
            estimated_files = total_fast_to_slow
            estimated_size = fast_size_before
        else:
            estimated_files = total_slow_to_fast
            estimated_size = slow_size_after

        print(f"Files processed: {attempts}")
        print(f"Files moved successfully: {moved} (estimated: {estimated_files})")
        print(f"Data moved: {format_size(bytes_moved)} (estimated: {format_size(estimated_size)})")

        # Show accuracy for this direction
        if estimated_files > 0:
            file_accuracy = (moved / estimated_files) * 100
            print(f"Migration accuracy: {file_accuracy:.1f}% files")
        if estimated_size > 0:
            size_accuracy = (bytes_moved / estimated_size) * 100
            print(f"                    {size_accuracy:.1f}% data")

        if moved > 0:
            overall_speed = bytes_moved / total_migration_time if total_migration_time > 0 else 0
            print(f"Average file transfer time: {format_time(avg_time)}")
            print(f"Overall transfer speed: {format_speed(overall_speed)}")

        # Failure/skip breakdown
        failures = (
            stats['skipped_size'][direction] +
            stats['skipped_recycled'][direction] +
            stats['skipped_deleted'][direction] +
            stats['skipped_invalid'][direction] +
            stats['failed_db'][direction] +
            stats['failed_not_ready'][direction] +
            stats['failed_after_rsync'][direction] +
            stats['failed_rsync'][direction]
        )

        if failures > 0:
            print(f"Skipped/Failed: {failures} files")
            if stats['skipped_size'][direction] > 0:
                print(f"  - Size threshold: {stats['skipped_size'][direction]}")
            if stats['skipped_recycled'][direction] > 0:
                print(f"  - Recycled status: {stats['skipped_recycled'][direction]}")
            if stats['skipped_deleted'][direction] > 0:
                print(f"  - Deleted status: {stats['skipped_deleted'][direction]}")
            if stats['skipped_invalid'][direction] > 0:
                print(f"  - Invalid qcow2: {stats['skipped_invalid'][direction]}")
            if stats['failed_db'][direction] > 0:
                print(f"  - Database errors: {stats['failed_db'][direction]}")
            if stats['failed_not_ready'][direction] > 0:
                print(f"  - Not ready: {stats['failed_not_ready'][direction]}")
            if stats['failed_after_rsync'][direction] > 0:
                print(f"  - Post-rsync failures: {stats['failed_after_rsync'][direction]}")
            if stats['failed_rsync'][direction] > 0:
                print(f"  - Rsync failures: {stats['failed_rsync'][direction]}")

        print()

    # Overall summary
    total_moved = stats['files_moved'][fast_to_slow_name] + stats['files_moved'][slow_to_fast_name]
    total_bytes_moved = stats['bytes_moved'][fast_to_slow_name] + stats['bytes_moved'][slow_to_fast_name]
    total_attempts = stats['attempts'][fast_to_slow_name] + stats['attempts'][slow_to_fast_name]

    print("--- OVERALL SUMMARY ---")
    print(f"Total files processed: {total_attempts}")
    print(f"Total files moved: {total_moved}")
    print(f"Total data moved: {format_size(total_bytes_moved)}")

    if total_bytes_moved > 0 and total_migration_time > 0:
        overall_speed = total_bytes_moved / total_migration_time
        print(f"Average migration speed: {format_speed(overall_speed)}")
        efficiency = (total_moved / total_attempts * 100) if total_attempts > 0 else 0
        print(f"Migration efficiency: {efficiency:.1f}% ({total_moved}/{total_attempts})")

    print("="*80)
    print("All done")


if __name__ == "__main__":
    main()
