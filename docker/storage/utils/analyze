#!/usr/bin/env python3

#
#   Copyright © 2024 Josep Maria Viñolas Auquer, Alberto Larraz Dalmases
#
#   This file is part of IsardVDI.
#
#   IsardVDI is free software: you can redistribute it and/or modify
#   it under the terms of the GNU Affero General Public License as published by
#   the Free Software Foundation, either version 3 of the License, or (at your
#   option) any later version.
#
#   IsardVDI is distributed in the hope that it will be useful, but WITHOUT ANY
#   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
#   FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
#   details.
#
#   You should have received a copy of the GNU Affero General Public License
#   along with IsardVDI. If not, see <https://www.gnu.org/licenses/>.
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import argparse
import datetime
import json
import os
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from pathlib import Path
from time import time

from rethinkdb import r


@contextmanager
def get_conn():
    connection = None
    try:
        connection = r.connect(
            host=os.environ.get("RETHINKDB_HOST", "isard-db"),
            port=28015,
            db="isard",
        )
        yield connection
    except r.errors.ReqlDriverError as e:
        print(f"Connection failed: {e}")
        raise
    finally:
        if connection:
            connection.close()


from pathlib import Path

from tabulate import tabulate


def get_output_dir():
    """Create and return timestamped output directory."""
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M")
    output_dir = Path(f"/logs/analyze/{timestamp}")
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


def write_file_list(output_dir, filename, file_list):
    """Write a list of file paths to a text file."""
    filepath = output_dir / filename
    with open(filepath, "w") as f:
        for item in sorted(file_list):
            f.write(f"{item}\n")
    return len(file_list)


def count_templates(file_list):
    """Count files in /isard/templates directory."""
    return len([f for f in file_list if "/isard/templates" in str(f)])


def size(paths):
    total_size = 0
    for path in paths:
        # Ensure it's a Path object
        p = Path(path)
        if p.is_file():  # Check if the path is a file
            total_size += p.stat().st_size
        elif p.is_dir():  # If it's a directory, sum sizes of files recursively
            total_size += sum(f.stat().st_size for f in p.rglob("*") if f.is_file())
    return round(
        total_size / (1024**3), 2
    )  # Convert bytes to GB and round to 2 decimals


def format_time_remaining(seconds):
    """Format seconds into human-readable time string."""
    if seconds < 60:
        return f"{int(seconds)}s"
    elif seconds < 3600:
        minutes = int(seconds / 60)
        secs = int(seconds % 60)
        return f"{minutes}m {secs}s"
    else:
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours}h {minutes}m"


def modification(file_path):
    file = Path(file_path)
    if not file.is_file():
        raise ValueError(f"{file_path} is not a valid file.")
    # Get the modification time as a timestamp
    timestamp = file.stat().st_mtime
    # Convert the timestamp to a datetime object
    modification_time = datetime.datetime.fromtimestamp(timestamp)
    return modification_time


def older_first(storage_paths):
    # Convert storage paths to Path objects and ensure they are valid
    storage_paths = [Path(path) for path in storage_paths if Path(path).exists()]

    # Sort by modification time
    sorted_storages = sorted(
        storage_paths,
        key=lambda p: p.stat().st_mtime,  # Use modification time for sorting
    )
    return sorted_storages


def get_qcow_info(file_path):
    """Get Qcow2 file metadata using qemu-img, suppressing output and errors."""
    try:
        result = subprocess.run(
            ["qemu-img", "info", "-U", "--output=json", file_path],
            stdout=subprocess.PIPE,  # Suppress stdout
            stderr=subprocess.PIPE,  # Suppress stderr
            text=True,
            check=True,
        )
        # Parse JSON output
        info = json.loads(result.stdout)  # Use json.loads to safely parse the output
        # Use full-backing-filename (resolved absolute path) instead of backing-filename
        # which could be relative and cause path matching issues
        return info.get("full-backing-filename") or info.get("backing-filename")
    except subprocess.CalledProcessError:
        return None


def get_qcow_full_info(file_path):
    """Get complete Qcow2 file metadata including actual-size."""
    try:
        result = subprocess.run(
            ["qemu-img", "info", "-U", "--output=json", file_path],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=True,
        )
        info = json.loads(result.stdout)
        return {
            "actual-size": info.get("actual-size", 0),
            "virtual-size": info.get("virtual-size", 0),
            "backing-filename": info.get("backing-filename"),
        }
    except (subprocess.CalledProcessError, json.JSONDecodeError, FileNotFoundError):
        return None


def analyze_qcow_dependencies(qcow_files):
    """Analyze dependencies and categorize files."""
    dependency_map = {}
    reverse_map = {}
    total_files = len(qcow_files)
    processed = 0
    start_time = time()

    # Build dependency and reverse maps
    for qcow in qcow_files:
        backing_file = get_qcow_info(qcow)
        dependency_map[qcow] = backing_file

        # Build reverse map for derivatives
        if backing_file:
            reverse_map.setdefault(backing_file, []).append(qcow)

        processed += 1

        # Show progress every 50 files or at completion
        if processed % 50 == 0 or processed == total_files:
            elapsed = time() - start_time
            if processed > 0 and processed < total_files:
                avg_time = elapsed / processed
                remaining = avg_time * (total_files - processed)
                percentage = (processed / total_files) * 100
                print(f"  - Analyzing dependencies: {processed}/{total_files} files ({percentage:.1f}%) - Est. {format_time_remaining(remaining)} remaining")
            elif processed == total_files:
                print(f"  - Analyzing dependencies: {processed}/{total_files} files (100.0%) - Done")

    # Identify orphans and files without derivatives
    wo_backing = [file for file, backing in dependency_map.items() if backing is None]
    wo_derivatives = [file for file in qcow_files if file not in reverse_map]

    return {
        "wo_backing": wo_backing,
        "wo_derivatives": wo_derivatives,
        "dependency_map": dependency_map,
        "reverse_map": reverse_map,
    }


def get_backing_chain(file_path):
    """Get the full backing chain for a QCOW2 image, suppressing output and errors."""
    try:
        result = subprocess.run(
            ["qemu-img", "info", "-U", "--backing-chain", file_path],
            stdout=subprocess.PIPE,  # Suppress stdout
            stderr=subprocess.PIPE,  # Suppress stderr
            text=True,
            check=True,
        )
        return result.stdout  # Return the result if no error
    except subprocess.CalledProcessError:
        return None


def parse_backing_chain(output):
    """Parse the backing chain output to identify missing files."""
    broken_files = []
    lines = output.splitlines()

    # Iterate over each line in the output
    for line in lines:
        if "backing file" in line:
            # Extract the backing file path from the line
            backing_file = line.split(":")[1].strip()

            # Check if the file exists
            if not os.path.exists(backing_file):
                broken_files.append(backing_file)  # Add to broken list if missing
    return broken_files


def analyze_qcow_integrity(qcow_files):
    """Analyze the integrity of a list of Qcow2 files and check for broken backing chains."""
    broken_files = []
    total_files = len(qcow_files)
    processed = 0
    start_time = time()

    for qcow in qcow_files:
        if get_backing_chain(qcow) is None:
            broken_files.append(qcow)
        processed += 1

        # Show progress every 50 files or at completion
        if processed % 50 == 0 or processed == total_files:
            elapsed = time() - start_time
            if processed > 0 and processed < total_files:
                avg_time = elapsed / processed
                remaining = avg_time * (total_files - processed)
                percentage = (processed / total_files) * 100
                print(f"  - Checking integrity: {processed}/{total_files} files ({percentage:.1f}%) - Est. {format_time_remaining(remaining)} remaining")
            elif processed == total_files:
                print(f"  - Checking integrity: {processed}/{total_files} files (100.0%) - Done")

    return broken_files


### DATABASE


def get_db_storages():
    with get_conn() as conn:
        return list(
            r.table("storage")
            .filter(lambda storage: storage["status"] != "deleted")
            .merge(
                {
                    "path": r.row["directory_path"]
                    + "/"
                    + r.row["id"]
                    + "."
                    + r.row["type"]
                }
            )
            .pluck("path")["path"]
            .run(conn)
        )


def get_db_storages_with_size():
    """Get storages from database with qemu-img-info.actual-size for comparison."""
    with get_conn() as conn:
        return list(
            r.table("storage")
            .filter(lambda storage: storage["status"] != "deleted")
            .merge(
                {
                    "path": r.row["directory_path"]
                    + "/"
                    + r.row["id"]
                    + "."
                    + r.row["type"],
                    "db_actual_size": r.row["qemu-img-info"]["actual-size"].default(0),
                }
            )
            .pluck("id", "path", "status", "db_actual_size")
            .run(conn)
        )


def get_volatile_storages():
    base_path = "/isard"
    return [
        str(file) for file in Path(base_path + "/volatile").rglob("*") if file.is_file()
    ]


def get_system_storages():
    base_path = "/isard"

    print("  - Scanning groups directory...")
    groups = [
        str(file)
        for file in Path(base_path + "/groups").rglob("*")
        if file.is_file()
    ]

    print("  - Scanning templates directory...")
    templates = [
        str(file)
        for file in Path(base_path + "/templates").rglob("*")
        if file.is_file()
    ]

    print("  - Scanning volatile directory...")
    volatile = [
        str(file)
        for file in Path(base_path + "/volatile").rglob("*")
        if file.is_file()
    ]

    print("  - Scanning storage_pools directory...")
    storage_pools = [
        str(file)
        for file in Path(base_path + "/storage_pools").rglob("*")
        if file.is_file()
    ]

    return groups + templates + volatile + storage_pools


### SIZE COMPARISON


def get_file_size_worker(file_path):
    """Worker function to get actual size for a single file."""
    info = get_qcow_full_info(file_path)
    if info:
        return file_path, info["actual-size"]
    return file_path, None


def compare_storage_sizes(db_storages_with_size, fs_files, threshold_gb=0.1):
    """Compare database qemu-img-info.actual-size with real file system sizes.

    Args:
        db_storages_with_size: List of dicts from get_db_storages_with_size()
        fs_files: List of file paths from get_system_storages()
        threshold_gb: Only report discrepancies above this threshold (in GB)

    Returns:
        Dict with comparison results
    """
    # Create lookup dict for DB storages
    db_lookup = {s["path"]: s for s in db_storages_with_size}

    # Filter to only qcow2 files that exist in DB
    qcow_files = [f for f in fs_files if f.endswith(".qcow2") and f in db_lookup]

    print(f"  - Getting actual sizes for {len(qcow_files)} qcow2 files...")

    # Get actual sizes from file system in parallel
    fs_sizes = {}
    total_files = len(qcow_files)
    processed = 0
    start_time = time()

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(get_file_size_worker, f): f for f in qcow_files}

        for future in as_completed(futures):
            file_path, actual_size = future.result()
            fs_sizes[file_path] = actual_size
            processed += 1
            if processed % 100 == 0 or processed == total_files:
                elapsed = time() - start_time
                if processed > 0 and processed < total_files:
                    avg_time = elapsed / processed
                    remaining = avg_time * (total_files - processed)
                    percentage = (processed / total_files) * 100
                    print(f"  - Progress: {processed}/{total_files} files ({percentage:.1f}%) - Est. {format_time_remaining(remaining)} remaining")
                elif processed == total_files:
                    print(f"  - Progress: {processed}/{total_files} files (100.0%) - Done")

    # Compare sizes
    matched = []
    discrepancies = []
    missing_in_fs = []
    total_db_size = 0
    total_fs_size = 0

    for db_storage in db_storages_with_size:
        path = db_storage["path"]
        db_size = db_storage["db_actual_size"]
        total_db_size += db_size

        if path in fs_sizes:
            fs_size = fs_sizes[path]
            if fs_size is not None:
                total_fs_size += fs_size
                diff_bytes = abs(fs_size - db_size)
                diff_gb = diff_bytes / (1024**3)

                if diff_gb >= threshold_gb:
                    discrepancies.append({
                        "path": path,
                        "id": db_storage["id"],
                        "db_size_gb": round(db_size / (1024**3), 2),
                        "fs_size_gb": round(fs_size / (1024**3), 2),
                        "diff_gb": round(diff_gb, 2),
                        "diff_pct": round((diff_bytes / max(db_size, 1)) * 100, 1),
                    })
                else:
                    matched.append(path)
            else:
                missing_in_fs.append(path)

    # Sort discrepancies by size difference (largest first)
    discrepancies.sort(key=lambda x: x["diff_gb"], reverse=True)

    return {
        "matched": matched,
        "discrepancies": discrepancies,
        "missing_in_fs": missing_in_fs,
        "total_db_size_gb": round(total_db_size / (1024**3), 2),
        "total_fs_size_gb": round(total_fs_size / (1024**3), 2),
        "total_diff_gb": round(abs(total_fs_size - total_db_size) / (1024**3), 2),
    }


def print_size_comparison_report(comparison, threshold_gb=0.1):
    """Print formatted size comparison report."""
    print("\n" + "=" * 80)
    print("STORAGE SIZE COMPARISON REPORT")
    print("=" * 80)

    # Summary table
    summary_data = [
        ["Metric", "Value"],
        ["Total DB Size (GB)", comparison["total_db_size_gb"]],
        ["Total FS Size (GB)", comparison["total_fs_size_gb"]],
        ["Total Difference (GB)", comparison["total_diff_gb"]],
        ["Matched Files", len(comparison["matched"])],
        [f"Discrepancies (>{threshold_gb} GB)", len(comparison["discrepancies"])],
        ["Missing in FS", len(comparison["missing_in_fs"])],
    ]

    print("\nSummary:")
    print(
        tabulate(
            summary_data,
            headers="firstrow",
            tablefmt="grid",
            stralign="left",
            numalign="right",
        )
    )

    # Discrepancies table (show top 20)
    if comparison["discrepancies"]:
        print(f"\nTop Discrepancies (showing up to 20):")
        disc_data = [["Path", "DB Size (GB)", "FS Size (GB)", "Diff (GB)", "Diff (%)"]]
        for disc in comparison["discrepancies"][:20]:
            # Shorten path for display
            short_path = disc["path"].replace("/isard/", ".../")
            if len(short_path) > 60:
                short_path = "..." + short_path[-57:]
            disc_data.append([
                short_path,
                disc["db_size_gb"],
                disc["fs_size_gb"],
                disc["diff_gb"],
                disc["diff_pct"],
            ])

        print(
            tabulate(
                disc_data,
                headers="firstrow",
                tablefmt="grid",
                stralign="left",
                numalign="right",
            )
        )

        if len(comparison["discrepancies"]) > 20:
            print(f"\n  ... and {len(comparison['discrepancies']) - 20} more discrepancies")

    # Missing files
    if comparison["missing_in_fs"]:
        print(f"\nFiles in DB but failed to get size from FS ({len(comparison['missing_in_fs'])}):")
        for path in comparison["missing_in_fs"][:10]:
            print(f"  - {path}")
        if len(comparison["missing_in_fs"]) > 10:
            print(f"  ... and {len(comparison['missing_in_fs']) - 10} more")

    print("\n" + "=" * 80)


### MAIN EXECUTION

if __name__ == "__main__":
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Analyze IsardVDI storage integrity and optionally compare sizes."
    )
    parser.add_argument(
        "--compare-sizes",
        action="store_true",
        help="Compare database qemu-img-info.actual-size with real file sizes",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.1,
        help="Size difference threshold in GB for reporting discrepancies (default: 0.1)",
    )
    args = parser.parse_args()

t = time()
print(f"Starting storage analysis...")
print(f"Getting storages from database...")
db_storages = get_db_storages()
print(f" - Got {len(db_storages)} storages from database in {time()-t:.2f} seconds")
print(f"Getting storages from system...")
sys_storages = get_system_storages()
print(f" - Got {len(sys_storages)} storages from system in {time()-t:.2f} seconds")
print(f"Analyzing storages...")
qcow_storages = [s for s in sys_storages if s.endswith(".qcow2")]
non_qcow_storages = [s for s in sys_storages if not s.endswith(".qcow2")]
storages_in_db_not_in_system = set(db_storages) - set(qcow_storages)
storages_in_system_not_in_db = set(qcow_storages) - set(db_storages)
print(f" - Analyzed storages in {time()-t:.2f} seconds")
print(f"Analyzing qcow integrity...")
broken_chain = analyze_qcow_integrity(qcow_storages)  # files with broken backing chain
print(f" - Analyzed qcow integrity in {time()-t:.2f} seconds")
valid_qcow_storages = set(qcow_storages) - set(broken_chain)
print(f"Analyzing qcow dependencies...")
# Analyze ALL qcow files (including broken ones) to correctly identify derivatives
# A file with broken chain can still depend on a valid parent - we need to count that
qcow_dependencies = analyze_qcow_dependencies(qcow_storages)
print(f" - Analyzed qcow dependencies in {time()-t:.2f} seconds")

print(f"Finished in {time()-t:.2f} seconds, generating report...")

table_data = [
    ["Info", "Count", "Size (GB)"],
    ["Storages in db", len(db_storages), "-"],
    ["Storages in system", len(sys_storages), size(sys_storages)],
    ["Non-qcow2 files", len(non_qcow_storages), size(non_qcow_storages)],
    ["Storages in db not in system", len(storages_in_db_not_in_system), "-"],
    [
        "Storages in system not in db",
        len(storages_in_system_not_in_db),
        size(storages_in_system_not_in_db),
    ],
    ["Broken chain", len(broken_chain), size(broken_chain)],
    [
        "No backing file",
        len(qcow_dependencies["wo_backing"]),
        size(qcow_dependencies["wo_backing"]),
    ],
    [
        "No derivatives",
        len(qcow_dependencies["wo_derivatives"]),
        size(qcow_dependencies["wo_derivatives"]),
    ],
    [
        "Volatile storages",
        len(get_volatile_storages()),
        size(get_volatile_storages()),
    ],
]

print(
    tabulate(
        table_data,
        headers="firstrow",
        tablefmt="grid",
        stralign="center",
        numalign="center",
    )
)


table_data = [
    ["To delete", "What", "Count", "Size (GB)"],
    [
        "Storages in db not in system",
        "db storage&domains entries",
        len(storages_in_db_not_in_system),
        "-",
    ],
    ["Non-qcow2 files", "qcow files", len(non_qcow_storages), size(non_qcow_storages)],
    [
        "Volatile storages",
        "qcow files",
        len(get_volatile_storages()),
        size(get_volatile_storages()),
    ],
    ["Broken chain", "qcow files", len(broken_chain), size(broken_chain)],
    [
        "Storages in system not in db with no derivatives",
        "qcow files",
        len(set(qcow_dependencies["wo_derivatives"]) & storages_in_system_not_in_db),
        size(set(qcow_dependencies["wo_derivatives"]) & storages_in_system_not_in_db),
    ],
]

print(
    tabulate(
        table_data,
        headers="firstrow",
        tablefmt="grid",
        stralign="center",
        numalign="center",
    )
)
print(
    "Total size of storages that may be deleted: ",
    size(get_volatile_storages())
    + size(non_qcow_storages)
    + size(broken_chain)
    + size(set(qcow_dependencies["wo_derivatives"]) & storages_in_system_not_in_db),
)

# Export results to files
output_dir = get_output_dir()
print(f"\nExporting results to {output_dir}...")

# Calculate deletable files (system_not_in_db AND no_derivatives)
deletable = set(qcow_dependencies["wo_derivatives"]) & storages_in_system_not_in_db
volatile = get_volatile_storages()

# Export file lists
# ok_ prefix = informative only
# bad_ prefix = candidates to be deleted
# volatile_ prefix = volatile storage (can be deleted without problems)
files_info = [
    ("ok_db_not_in_system.txt", storages_in_db_not_in_system),
    ("ok_system_not_in_db.txt", storages_in_system_not_in_db),
    ("ok_no_backing_file.txt", qcow_dependencies["wo_backing"]),
    ("ok_no_derivatives.txt", qcow_dependencies["wo_derivatives"]),
    ("bad_broken_chain.txt", broken_chain),
    ("volatile_storage.txt", volatile),
    ("bad_non_qcow2.txt", non_qcow_storages),
    ("bad_deletable_orphans.txt", deletable),
]

for filename, file_list in files_info:
    write_file_list(output_dir, filename, file_list)

# Count template files in each list
templates_in_broken = count_templates(broken_chain)
templates_in_deletable = count_templates(deletable)
templates_in_system_not_db = count_templates(storages_in_system_not_in_db)
templates_in_non_qcow2 = count_templates(non_qcow_storages)
# Volatile files don't need template warnings - they're always safe to delete
total_templates_in_deletable = templates_in_broken + templates_in_deletable + templates_in_non_qcow2

# Generate README.md with summary tables
readme_content = f"""# Storage Analysis Report

Generated: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Summary

| Metric | Count |
|--------|-------|
| Storages in DB | {len(db_storages)} |
| Storages in system | {len(sys_storages)} |
| Qcow2 files | {len(qcow_storages)} |

## Informative Files (ok_)

| File | Count | Templates | Size (GB) | Description |
|------|-------|-----------|-----------|-------------|
| ok_db_not_in_system.txt | {len(storages_in_db_not_in_system)} | - | - | Paths in DB but missing on disk |
| ok_system_not_in_db.txt | {len(storages_in_system_not_in_db)} | {templates_in_system_not_db} | {size(storages_in_system_not_in_db)} | Files on disk not in DB |
| ok_no_backing_file.txt | {len(qcow_dependencies["wo_backing"])} | {count_templates(qcow_dependencies["wo_backing"])} | {size(qcow_dependencies["wo_backing"])} | Root images (no backing file) |
| ok_no_derivatives.txt | {len(qcow_dependencies["wo_derivatives"])} | {count_templates(qcow_dependencies["wo_derivatives"])} | {size(qcow_dependencies["wo_derivatives"])} | Leaf images (no derivatives) |

## Candidates to Delete (bad_)

| File | Count | Templates | Size (GB) | Description |
|------|-------|-----------|-----------|-------------|
| bad_broken_chain.txt | {len(broken_chain)} | {templates_in_broken} | {size(broken_chain)} | Files with broken backing chains |
| bad_non_qcow2.txt | {len(non_qcow_storages)} | {templates_in_non_qcow2} | {size(non_qcow_storages)} | Non-qcow2 files |
| bad_deletable_orphans.txt | {len(deletable)} | {templates_in_deletable} | {size(deletable)} | Safe to delete (not in DB + no derivatives) |

## Volatile Storage (volatile_)

| File | Count | Size (GB) | Description |
|------|-------|-----------|-------------|
| volatile_storage.txt | {len(volatile)} | {size(volatile)} | Volatile storage files (safe to delete) |

## Cleanup Summary

| Action | File | Count | Templates | Size (GB) |
|--------|------|-------|-----------|-----------|
| Delete files | bad_deletable_orphans.txt | {len(deletable)} | {templates_in_deletable} | {size(deletable)} |
| Delete files | bad_broken_chain.txt | {len(broken_chain)} | {templates_in_broken} | {size(broken_chain)} |
| Delete files | bad_non_qcow2.txt | {len(non_qcow_storages)} | {templates_in_non_qcow2} | {size(non_qcow_storages)} |
| Delete files | volatile_storage.txt | {len(volatile)} | - | {size(volatile)} |

**Total deletable space:** {size(deletable) + size(broken_chain) + size(volatile) + size(non_qcow_storages)} GB

{"⚠️  **WARNING: Some files are in /isard/templates - review carefully before deleting!**" if total_templates_in_deletable > 0 else ""}

## Sample Commands

{"⚠️  **WARNING:** Check for template files before running delete commands:" if total_templates_in_deletable > 0 else ""}
```bash
# Check for templates in any file before deleting
grep "/isard/templates" *.txt
```

### Delete broken chain files
{"⚠️  **WARNING: Contains " + str(templates_in_broken) + " template files! Review first:**" if templates_in_broken > 0 else ""}
```bash
{"grep '/isard/templates' bad_broken_chain.txt  # Review templates first!" if templates_in_broken > 0 else ""}
cat bad_broken_chain.txt | xargs rm -f
```

### Delete volatile files
```bash
cat volatile_storage.txt | xargs rm -f
```

### Delete orphan files (safe to delete)
{"⚠️  **WARNING: Contains " + str(templates_in_deletable) + " template files! Review first:**" if templates_in_deletable > 0 else ""}
```bash
{"grep '/isard/templates' bad_deletable_orphans.txt  # Review templates first!" if templates_in_deletable > 0 else ""}
cat bad_deletable_orphans.txt | xargs rm -f
```

### Delete non-qcow2 files
{"⚠️  **WARNING: Contains " + str(templates_in_non_qcow2) + " template files! Review first:**" if templates_in_non_qcow2 > 0 else ""}
```bash
{"grep '/isard/templates' bad_non_qcow2.txt  # Review templates first!" if templates_in_non_qcow2 > 0 else ""}
cat bad_non_qcow2.txt | xargs rm -f
```

### Delete all bad_ files at once
```bash
cat bad_*.txt | sort -u | xargs rm -f
```

### Delete all deletable files (bad_ + volatile_)
```bash
cat bad_*.txt volatile_*.txt | sort -u | xargs rm -f
```

### Get qemu-img info for files not in DB
```bash
while read f; do echo "=== $f ===" && qemu-img info -U "$f"; done < ok_system_not_in_db.txt
```

### Count files in each list
```bash
for f in *.txt; do echo "$f: $(wc -l < "$f")"; done
```

### Filter out templates before deleting
```bash
# Example: delete only non-template files from bad_broken_chain.txt
grep -v "/isard/templates" bad_broken_chain.txt | xargs rm -f
```
"""

with open(output_dir / "README.md", "w") as f:
    f.write(readme_content)

# Size comparison (if requested)
if args.compare_sizes:
    print("\n" + "=" * 80)
    print("STARTING SIZE COMPARISON ANALYSIS")
    print("=" * 80)
    print(f"Fetching storage data with qemu-img-info.actual-size from database...")
    size_comparison_start = time()
    db_storages_with_size = get_db_storages_with_size()
    print(
        f" - Got {len(db_storages_with_size)} storages from database in {time()-size_comparison_start:.2f} seconds"
    )

    print(f"Comparing database sizes with file system sizes (threshold: {args.threshold} GB)...")
    comparison_result = compare_storage_sizes(
        db_storages_with_size, sys_storages, args.threshold
    )
    print(f" - Comparison completed in {time()-size_comparison_start:.2f} seconds")

    print_size_comparison_report(comparison_result, args.threshold)

    # Export paths with size discrepancies (informative - need find task, not deletion)
    discrepancy_paths = [d["path"] for d in comparison_result["discrepancies"]]
    write_file_list(output_dir, "ok_size_discrepancy_paths.txt", discrepancy_paths)

    # Append size comparison section to README.md
    templates_in_discrepancies = count_templates(discrepancy_paths)
    size_section = f"""
## Size Comparison (--compare-sizes)

| Metric | Value |
|--------|-------|
| Total DB Size (GB) | {comparison_result["total_db_size_gb"]} |
| Total FS Size (GB) | {comparison_result["total_fs_size_gb"]} |
| Total Difference (GB) | {comparison_result["total_diff_gb"]} |
| Matched Files | {len(comparison_result["matched"])} |
| Discrepancies (>{args.threshold} GB) | {len(comparison_result["discrepancies"])} |

| File | Count | Templates | Description |
|------|-------|-----------|-------------|
| ok_size_discrepancy_paths.txt | {len(discrepancy_paths)} | {templates_in_discrepancies} | Paths where DB size differs from actual file size |

### Get qemu-img info for size discrepancies
{"⚠️  **WARNING: Contains " + str(templates_in_discrepancies) + " template files! Review first:**" if templates_in_discrepancies > 0 else ""}
```bash
{"grep '/isard/templates' ok_size_discrepancy_paths.txt  # Review templates first!" if templates_in_discrepancies > 0 else ""}
while read f; do echo "=== $f ===" && qemu-img info -U "$f"; done < ok_size_discrepancy_paths.txt
```
"""

    with open(output_dir / "README.md", "a") as f:
        f.write(size_section)

print(f"\nResults exported to: {output_dir}")
print(f"Total execution time: {time()-t:.2f} seconds")
